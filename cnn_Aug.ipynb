{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Siyuan He]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run my code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "4. Upload this .HTML file to your Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM3/cnn.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    one_hot_label = np.zeros((len(y),num_class))\n",
    "    for i,label in enumerate(y):\n",
    "        one_hot_label[i,label] = 1.0\n",
    "    return one_hot_label\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = np.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_datagen.fit(x_tr)\n",
    "train_generator = train_datagen.flow(x_tr,y_tr,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Activation('relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 0.0001 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1250/1250 [==============================] - 404s 324ms/step - loss: 1.8207 - acc: 0.3364 - val_loss: 1.3644 - val_acc: 0.5124\n",
      "Epoch 2/50\n",
      "1250/1250 [==============================] - 404s 323ms/step - loss: 1.5093 - acc: 0.4515 - val_loss: 1.2280 - val_acc: 0.5683\n",
      "Epoch 3/50\n",
      "1250/1250 [==============================] - 382s 305ms/step - loss: 1.3776 - acc: 0.4988 - val_loss: 1.1248 - val_acc: 0.6043\n",
      "Epoch 4/50\n",
      "1250/1250 [==============================] - 454s 363ms/step - loss: 1.2938 - acc: 0.5315 - val_loss: 1.1111 - val_acc: 0.6056\n",
      "Epoch 5/50\n",
      "1250/1250 [==============================] - 456s 365ms/step - loss: 1.2237 - acc: 0.5575 - val_loss: 1.0569 - val_acc: 0.6232\n",
      "Epoch 6/50\n",
      "1250/1250 [==============================] - 442s 354ms/step - loss: 1.1749 - acc: 0.5774 - val_loss: 0.9568 - val_acc: 0.6636\n",
      "Epoch 7/50\n",
      "1250/1250 [==============================] - 466s 373ms/step - loss: 1.1293 - acc: 0.5992 - val_loss: 0.9500 - val_acc: 0.6612\n",
      "Epoch 8/50\n",
      "1250/1250 [==============================] - 463s 370ms/step - loss: 1.0906 - acc: 0.6079 - val_loss: 0.9299 - val_acc: 0.6722\n",
      "Epoch 9/50\n",
      "1250/1250 [==============================] - 458s 366ms/step - loss: 1.0645 - acc: 0.6210 - val_loss: 0.8590 - val_acc: 0.6959\n",
      "Epoch 10/50\n",
      "1250/1250 [==============================] - 465s 372ms/step - loss: 1.0285 - acc: 0.6341 - val_loss: 0.8428 - val_acc: 0.7030\n",
      "Epoch 11/50\n",
      "1250/1250 [==============================] - 466s 372ms/step - loss: 0.9961 - acc: 0.6465 - val_loss: 0.7710 - val_acc: 0.7315\n",
      "Epoch 12/50\n",
      "1250/1250 [==============================] - 441s 353ms/step - loss: 0.9775 - acc: 0.6552 - val_loss: 0.9004 - val_acc: 0.6854\n",
      "Epoch 13/50\n",
      "1250/1250 [==============================] - 472s 378ms/step - loss: 0.9547 - acc: 0.6633 - val_loss: 0.7708 - val_acc: 0.7341\n",
      "Epoch 14/50\n",
      "1250/1250 [==============================] - 458s 367ms/step - loss: 0.9334 - acc: 0.6724 - val_loss: 0.7701 - val_acc: 0.7293\n",
      "Epoch 15/50\n",
      "1250/1250 [==============================] - 408s 326ms/step - loss: 0.9074 - acc: 0.6783 - val_loss: 0.7774 - val_acc: 0.7331\n",
      "Epoch 16/50\n",
      "1250/1250 [==============================] - 403s 322ms/step - loss: 0.8976 - acc: 0.6823 - val_loss: 0.7607 - val_acc: 0.7328\n",
      "Epoch 17/50\n",
      "1250/1250 [==============================] - 399s 320ms/step - loss: 0.8824 - acc: 0.6902 - val_loss: 0.7489 - val_acc: 0.7392\n",
      "Epoch 18/50\n",
      "1250/1250 [==============================] - 388s 310ms/step - loss: 0.8650 - acc: 0.6960 - val_loss: 0.6581 - val_acc: 0.7735\n",
      "Epoch 19/50\n",
      "1250/1250 [==============================] - 385s 308ms/step - loss: 0.8551 - acc: 0.7017 - val_loss: 0.7129 - val_acc: 0.7536\n",
      "Epoch 20/50\n",
      "1250/1250 [==============================] - 349s 279ms/step - loss: 0.8410 - acc: 0.7065 - val_loss: 0.6811 - val_acc: 0.7616\n",
      "Epoch 21/50\n",
      "1250/1250 [==============================] - 396s 317ms/step - loss: 0.8244 - acc: 0.7124 - val_loss: 0.6744 - val_acc: 0.7673\n",
      "Epoch 22/50\n",
      "1250/1250 [==============================] - 527s 421ms/step - loss: 0.8209 - acc: 0.7142 - val_loss: 0.7084 - val_acc: 0.7561\n",
      "Epoch 23/50\n",
      "1250/1250 [==============================] - 474s 379ms/step - loss: 0.8007 - acc: 0.7195 - val_loss: 0.7615 - val_acc: 0.7486\n",
      "Epoch 24/50\n",
      "1250/1250 [==============================] - 437s 350ms/step - loss: 0.8002 - acc: 0.7213 - val_loss: 0.6173 - val_acc: 0.7891\n",
      "Epoch 25/50\n",
      "1250/1250 [==============================] - 447s 358ms/step - loss: 0.7775 - acc: 0.7284 - val_loss: 0.7493 - val_acc: 0.7582\n",
      "Epoch 26/50\n",
      "1250/1250 [==============================] - 447s 358ms/step - loss: 0.7770 - acc: 0.7288 - val_loss: 0.6659 - val_acc: 0.7702\n",
      "Epoch 27/50\n",
      "1250/1250 [==============================] - 472s 378ms/step - loss: 0.7600 - acc: 0.7345 - val_loss: 0.6443 - val_acc: 0.7775\n",
      "Epoch 28/50\n",
      "1250/1250 [==============================] - 451s 361ms/step - loss: 0.7560 - acc: 0.7361 - val_loss: 0.6940 - val_acc: 0.7693\n",
      "Epoch 29/50\n",
      "1250/1250 [==============================] - 454s 363ms/step - loss: 0.7541 - acc: 0.7374 - val_loss: 0.6547 - val_acc: 0.7796\n",
      "Epoch 30/50\n",
      "1250/1250 [==============================] - 517s 414ms/step - loss: 0.7361 - acc: 0.7429 - val_loss: 0.6517 - val_acc: 0.7765\n",
      "Epoch 31/50\n",
      "1250/1250 [==============================] - 509s 407ms/step - loss: 0.7286 - acc: 0.7454 - val_loss: 0.6321 - val_acc: 0.7853\n",
      "Epoch 32/50\n",
      "1250/1250 [==============================] - 588s 470ms/step - loss: 0.7214 - acc: 0.7513 - val_loss: 0.6276 - val_acc: 0.7904\n",
      "Epoch 33/50\n",
      "1250/1250 [==============================] - 586s 469ms/step - loss: 0.7143 - acc: 0.7509 - val_loss: 0.6349 - val_acc: 0.7858\n",
      "Epoch 34/50\n",
      "1250/1250 [==============================] - 432s 346ms/step - loss: 0.7053 - acc: 0.7551 - val_loss: 0.6228 - val_acc: 0.7916\n",
      "Epoch 35/50\n",
      "1250/1250 [==============================] - 349s 280ms/step - loss: 0.7021 - acc: 0.7591 - val_loss: 0.6319 - val_acc: 0.7872\n",
      "Epoch 36/50\n",
      "1250/1250 [==============================] - 351s 281ms/step - loss: 0.6956 - acc: 0.7595 - val_loss: 0.5899 - val_acc: 0.8007\n",
      "Epoch 37/50\n",
      "1250/1250 [==============================] - 351s 281ms/step - loss: 0.6903 - acc: 0.7618 - val_loss: 0.6528 - val_acc: 0.7843\n",
      "Epoch 38/50\n",
      "1250/1250 [==============================] - 353s 282ms/step - loss: 0.6829 - acc: 0.7630 - val_loss: 0.5578 - val_acc: 0.8093\n",
      "Epoch 39/50\n",
      "1250/1250 [==============================] - 351s 281ms/step - loss: 0.6737 - acc: 0.7680 - val_loss: 0.5544 - val_acc: 0.8123\n",
      "Epoch 40/50\n",
      "1250/1250 [==============================] - 340s 272ms/step - loss: 0.6683 - acc: 0.7701 - val_loss: 0.6438 - val_acc: 0.7953\n",
      "Epoch 41/50\n",
      "1250/1250 [==============================] - 369s 295ms/step - loss: 0.6593 - acc: 0.7719 - val_loss: 0.5582 - val_acc: 0.8117\n",
      "Epoch 42/50\n",
      "1250/1250 [==============================] - 390s 312ms/step - loss: 0.6610 - acc: 0.7727 - val_loss: 0.5985 - val_acc: 0.7977\n",
      "Epoch 43/50\n",
      "1250/1250 [==============================] - 386s 309ms/step - loss: 0.6476 - acc: 0.7777 - val_loss: 0.5550 - val_acc: 0.8126\n",
      "Epoch 44/50\n",
      "1250/1250 [==============================] - 386s 309ms/step - loss: 0.6488 - acc: 0.7775 - val_loss: 0.5844 - val_acc: 0.8057\n",
      "Epoch 45/50\n",
      "1250/1250 [==============================] - 386s 309ms/step - loss: 0.6456 - acc: 0.7744 - val_loss: 0.5575 - val_acc: 0.8136\n",
      "Epoch 46/50\n",
      "1250/1250 [==============================] - 385s 308ms/step - loss: 0.6350 - acc: 0.7817 - val_loss: 0.5968 - val_acc: 0.8017\n",
      "Epoch 47/50\n",
      "1250/1250 [==============================] - 386s 309ms/step - loss: 0.6266 - acc: 0.7818 - val_loss: 0.5838 - val_acc: 0.8099\n",
      "Epoch 48/50\n",
      "1250/1250 [==============================] - 390s 312ms/step - loss: 0.6271 - acc: 0.7845 - val_loss: 0.5840 - val_acc: 0.8046\n",
      "Epoch 49/50\n",
      "1250/1250 [==============================] - 391s 313ms/step - loss: 0.6205 - acc: 0.7857 - val_loss: 0.6270 - val_acc: 0.7907\n",
      "Epoch 50/50\n",
      "1250/1250 [==============================] - 390s 312ms/step - loss: 0.6143 - acc: 0.7865 - val_loss: 0.5411 - val_acc: 0.8187\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=len(x_tr) / batch_size , epochs=50,validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgUiInJgUCAiIgcGBSIicvh/ZbA2/PaAbvkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d1bea7f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1250/1250 [==============================] - 351s 281ms/step - loss: 1.8292 - acc: 0.3331\n",
      "Epoch 2/50\n",
      "1250/1250 [==============================] - 355s 284ms/step - loss: 1.5187 - acc: 0.4482\n",
      "Epoch 3/50\n",
      "1250/1250 [==============================] - 322s 258ms/step - loss: 1.3908 - acc: 0.4965\n",
      "Epoch 4/50\n",
      "1250/1250 [==============================] - 319s 255ms/step - loss: 1.2952 - acc: 0.5329\n",
      "Epoch 5/50\n",
      "1250/1250 [==============================] - 332s 266ms/step - loss: 1.2313 - acc: 0.5588\n",
      "Epoch 6/50\n",
      "1250/1250 [==============================] - 352s 281ms/step - loss: 1.1768 - acc: 0.5771\n",
      "Epoch 7/50\n",
      "1250/1250 [==============================] - 339s 271ms/step - loss: 1.1440 - acc: 0.5924\n",
      "Epoch 8/50\n",
      "1250/1250 [==============================] - 336s 269ms/step - loss: 1.0993 - acc: 0.6081\n",
      "Epoch 9/50\n",
      "1250/1250 [==============================] - 328s 262ms/step - loss: 1.0696 - acc: 0.6200\n",
      "Epoch 10/50\n",
      "1250/1250 [==============================] - 325s 260ms/step - loss: 1.0344 - acc: 0.6318\n",
      "Epoch 11/50\n",
      "1250/1250 [==============================] - 331s 264ms/step - loss: 1.0116 - acc: 0.6398\n",
      "Epoch 12/50\n",
      "1250/1250 [==============================] - 403s 322ms/step - loss: 0.9756 - acc: 0.6533\n",
      "Epoch 13/50\n",
      "1250/1250 [==============================] - 439s 351ms/step - loss: 0.9736 - acc: 0.6541\n",
      "Epoch 14/50\n",
      "1250/1250 [==============================] - 419s 335ms/step - loss: 0.9344 - acc: 0.6681\n",
      "Epoch 15/50\n",
      "1250/1250 [==============================] - 421s 337ms/step - loss: 0.9259 - acc: 0.6720\n",
      "Epoch 16/50\n",
      "1250/1250 [==============================] - 360s 288ms/step - loss: 0.8972 - acc: 0.6839\n",
      "Epoch 17/50\n",
      "1250/1250 [==============================] - 343s 274ms/step - loss: 0.8832 - acc: 0.6886\n",
      "Epoch 18/50\n",
      "1250/1250 [==============================] - 344s 275ms/step - loss: 0.8724 - acc: 0.6914\n",
      "Epoch 19/50\n",
      "1250/1250 [==============================] - 344s 275ms/step - loss: 0.8614 - acc: 0.6982\n",
      "Epoch 20/50\n",
      "1250/1250 [==============================] - 325s 260ms/step - loss: 0.8458 - acc: 0.7048\n",
      "Epoch 21/50\n",
      "1250/1250 [==============================] - 321s 257ms/step - loss: 0.8319 - acc: 0.7094\n",
      "Epoch 22/50\n",
      "1250/1250 [==============================] - 321s 257ms/step - loss: 0.8133 - acc: 0.7175\n",
      "Epoch 23/50\n",
      "1250/1250 [==============================] - 320s 256ms/step - loss: 0.8111 - acc: 0.7168\n",
      "Epoch 24/50\n",
      "1250/1250 [==============================] - 318s 255ms/step - loss: 0.7887 - acc: 0.7268\n",
      "Epoch 25/50\n",
      "1250/1250 [==============================] - 307s 245ms/step - loss: 0.7857 - acc: 0.7256\n",
      "Epoch 26/50\n",
      "1250/1250 [==============================] - 307s 246ms/step - loss: 0.7773 - acc: 0.7289\n",
      "Epoch 27/50\n",
      "1250/1250 [==============================] - 307s 246ms/step - loss: 0.7665 - acc: 0.7305\n",
      "Epoch 28/50\n",
      "1250/1250 [==============================] - 307s 246ms/step - loss: 0.7528 - acc: 0.7361\n",
      "Epoch 29/50\n",
      "1250/1250 [==============================] - 307s 246ms/step - loss: 0.7463 - acc: 0.7405\n",
      "Epoch 30/50\n",
      "1250/1250 [==============================] - 307s 246ms/step - loss: 0.7468 - acc: 0.7381\n",
      "Epoch 31/50\n",
      "1250/1250 [==============================] - 308s 247ms/step - loss: 0.7372 - acc: 0.7469\n",
      "Epoch 32/50\n",
      "1250/1250 [==============================] - 312s 249ms/step - loss: 0.7229 - acc: 0.7496\n",
      "Epoch 33/50\n",
      "1250/1250 [==============================] - 336s 269ms/step - loss: 0.7253 - acc: 0.7494\n",
      "Epoch 34/50\n",
      "1250/1250 [==============================] - 330s 264ms/step - loss: 0.7090 - acc: 0.7564\n",
      "Epoch 35/50\n",
      "1250/1250 [==============================] - 330s 264ms/step - loss: 0.7015 - acc: 0.7602\n",
      "Epoch 36/50\n",
      "1250/1250 [==============================] - 316s 253ms/step - loss: 0.6961 - acc: 0.7601\n",
      "Epoch 37/50\n",
      "1250/1250 [==============================] - 320s 256ms/step - loss: 0.6895 - acc: 0.7595\n",
      "Epoch 38/50\n",
      "1250/1250 [==============================] - 318s 254ms/step - loss: 0.6861 - acc: 0.7622\n",
      "Epoch 39/50\n",
      "1250/1250 [==============================] - 315s 252ms/step - loss: 0.6698 - acc: 0.7661\n",
      "Epoch 40/50\n",
      "1250/1250 [==============================] - 316s 253ms/step - loss: 0.6758 - acc: 0.7654\n",
      "Epoch 41/50\n",
      "1250/1250 [==============================] - 315s 252ms/step - loss: 0.6711 - acc: 0.7667\n",
      "Epoch 42/50\n",
      "1250/1250 [==============================] - 315s 252ms/step - loss: 0.6639 - acc: 0.7709\n",
      "Epoch 43/50\n",
      "1250/1250 [==============================] - 324s 259ms/step - loss: 0.6606 - acc: 0.7725\n",
      "Epoch 44/50\n",
      "1250/1250 [==============================] - 327s 262ms/step - loss: 0.6564 - acc: 0.7722\n",
      "Epoch 45/50\n",
      "1250/1250 [==============================] - 323s 259ms/step - loss: 0.6424 - acc: 0.7784\n",
      "Epoch 46/50\n",
      "1250/1250 [==============================] - 312s 250ms/step - loss: 0.6419 - acc: 0.7785\n",
      "Epoch 47/50\n",
      "1250/1250 [==============================] - 312s 249ms/step - loss: 0.6318 - acc: 0.7808\n",
      "Epoch 48/50\n",
      "1250/1250 [==============================] - 313s 250ms/step - loss: 0.6336 - acc: 0.7807\n",
      "Epoch 49/50\n",
      "1250/1250 [==============================] - 313s 250ms/step - loss: 0.6291 - acc: 0.7819\n",
      "Epoch 50/50\n",
      "1250/1250 [==============================] - 312s 250ms/step - loss: 0.6309 - acc: 0.7821\n"
     ]
    }
   ],
   "source": [
    "# <Train your model on the entire training set (50K samples)>\n",
    "# <Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
    "# <Do NOT use the validation_data option (because now you do not have validation data)>\n",
    "# ...\n",
    "\n",
    "train_datagen.fit(x_train)\n",
    "train_generator_all = train_datagen.flow(x_train,y_train_vec,batch_size=batch_size)\n",
    "\n",
    "learning_rate = 0.0001 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(train_generator_all,\n",
    "                    steps_per_epoch=len(x_tr) / batch_size , epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 30s 3ms/step\n",
      "loss = 0.565274450493\n",
      "accuracy = 0.8163\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
